{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "import sys\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self, num_of_houses):\n",
    "        # read from train list and concatenate the data\n",
    "        for i in range(len(num_of_houses)):\n",
    "            data = pd.read_csv(self.config.data_path + str(num_of_houses[i]) + '_compressed.csv')\n",
    "            if i == 0:\n",
    "                all_data = data\n",
    "            else:\n",
    "                all_data = pd.concat([all_data, data], axis=0)\n",
    "        return all_data\n",
    "    \n",
    "    # write a function using MinMax scalar to normalize the data\n",
    "    def normalize_data(self, data):\n",
    "        scaler = MinMaxScaler()\n",
    "        # data = scaler.fit_transform(data)\n",
    "        data = data.to_numpy()\n",
    "        # scaler = 0\n",
    "        return data, scaler\n",
    "    \n",
    "    def segment_ev_load_data(self, data, window_length, num_of_houses):\n",
    "        \n",
    "        data = torch.from_numpy(data).to(self.config.device).float()\n",
    "        segmented_data = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for i in range(len(num_of_houses)):\n",
    "            per_house_data = data[data[:, 0] == num_of_houses[i]]\n",
    "            # Calculate the number of segments\n",
    "            num_segments = (len(per_house_data) - window_length) + 1\n",
    "            # Perform sliding window segmentation\n",
    "            for start in range(num_segments):\n",
    "                segmented_data.append(per_house_data[start:start + window_length, 2:])\n",
    "                ground_truth.append(per_house_data[start:start + window_length, 1:2])\n",
    "\n",
    "        # Perform sliding window segmentation\n",
    "        # for start in range(num_segments):\n",
    "        #     segmented_data = torch.cat(segmented_data, data[start:start + window_length, 1:])\n",
    "        #     ground_truth = torch.cat(ground_truth, data[start:start + window_length, 0:1])\n",
    "\n",
    "        # segmented_3d = segmented_data.reshape(segmented_data.shape[0], segmented_data.shape[1], -1)\n",
    "        # ground_truth = ground_truth.reshape(ground_truth.shape[0], ground_truth.shape[1], -1)\n",
    "\n",
    "        segmented_data = torch.stack(segmented_data)\n",
    "        ground_truth = torch.stack(ground_truth)\n",
    "\n",
    "        segmented_data = segmented_data[: -(segmented_data.shape[0] % self.config.batch_size)]\n",
    "        ground_truth = ground_truth[: -(ground_truth.shape[0] % self.config.batch_size)]\n",
    "        return segmented_data, ground_truth\n",
    "        \n",
    "    #write a function for train test loader\n",
    "    def get_data_loader(self, data_preprocess, columns):\n",
    "        data = self.load_data(self.config.train)\n",
    "        data = data_preprocess.preprocess_data(data)\n",
    "        id_col = data['dataid'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "        train_data, scaler_train = self.normalize_data(data[columns])\n",
    "        train_data = np.concatenate((id_col, train_data), axis=1)\n",
    "        train_data, train_ground_truth = self.segment_ev_load_data(train_data, self.config.lag_size, self.config.train)\n",
    "\n",
    "        data = self.load_data(self.config.test)\n",
    "        data = data_preprocess.preprocess_data(data)\n",
    "        id_col = data['dataid'].to_numpy().reshape(-1, 1)\n",
    "        test_data, scalar_test = self.normalize_data(data[columns])\n",
    "        test_data = np.concatenate((id_col, test_data), axis=1)\n",
    "        test_data, test_ground_truth = self.segment_ev_load_data(test_data, self.config.lag_size, self.config.test)\n",
    "        # test_data, test_ground_truth = self.segment_ev_load_data(test_data, self.config.lag_size)\n",
    "\n",
    "        # train_dataset = CustomDataset(train_data, train_ground_truth)\n",
    "        # test_dataset = CustomDataset(test_data, test_ground_truth)\n",
    "\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False)\n",
    "\n",
    "        return train_data, train_ground_truth, test_data, test_ground_truth, scaler_train, scalar_test\n",
    "        # return train_loader, test_loader, scaler_train, scalar_test\n",
    "    \n",
    "    def split_train_test(self, data):\n",
    "        train_data = data[data['local_15min'] < self.config.split_date]\n",
    "        test_data = data[data['local_15min'] >= self.config.split_date]\n",
    "        return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
